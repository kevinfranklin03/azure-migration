{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b829d249-5655-4427-8a4e-f8e3d316cf7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver → Gold: Date Dimension\n",
    "\n",
    "## Purpose\n",
    "Create calendar dimension table for time-based analytics\n",
    "\n",
    "## Source\n",
    "- Generated from fact_orders date range\n",
    "\n",
    "## Output\n",
    "- Gold: `dim_date`\n",
    "- Grain: One row per date\n",
    "- Range: 2016-09-04 to 2018-10-17 (covers all orders)\n",
    "\n",
    "## Attributes\n",
    "- Date components (year, quarter, month, day, weekday)\n",
    "- Fiscal periods\n",
    "- Business day flags\n",
    "- Month/quarter names\n",
    "\n",
    "**Author:** Kevin  \n",
    "**Date:** Feb 9, 2026\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ffea4a-cdc2-4d63-a5e0-1cb740d818d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config loaded\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, date_format, dayofmonth, dayofweek, dayofyear,\n",
    "    weekofyear, year, month, quarter, when, current_timestamp,\n",
    "    expr, sequence, explode, to_date\n",
    ")\n",
    "\n",
    "storage_account_name = \"stgolistmigration\"\n",
    "account_key = \"\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "\n",
    "def get_gold_path(table):\n",
    "    return f\"abfss://gold@{storage_account_name}.dfs.core.windows.net/{table}/\"\n",
    "\n",
    "print(\"✅ Config loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974aa467-5069-470d-b347-187f51816e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC5 Generating date dimension...\n✅ Generated: 852 dates\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDCC5 Generating date dimension...\")\n",
    "\n",
    "# Generate date range from 2016-09-01 to 2018-12-31 (covers all Olist orders)\n",
    "df_dates = spark.sql(\"\"\"\n",
    "    SELECT explode(sequence(to_date('2016-09-01'), to_date('2018-12-31'), interval 1 day)) as date_key\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✅ Generated: {df_dates.count():,} dates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99155e4-20ae-44e7-b7a8-bf1ab7cc42af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Adding date attributes...\n✅ Date dimension created: 852 dates\n-RECORD 0----------------------------------------\n date_key           | 2016-09-01                 \n date_id            | 20160901                   \n year               | 2016                       \n quarter            | 3                          \n month              | 9                          \n month_name         | September                  \n day                | 1                          \n day_of_week        | 5                          \n day_name           | Thursday                   \n day_of_year        | 245                        \n week_of_year       | 35                         \n is_weekend         | false                      \n quarter_name       | Q3                         \n year_month         | 2016-09                    \n year_quarter       | 2016-Q3                    \n dim_load_timestamp | 2026-02-09 13:23:34.421156 \n-RECORD 1----------------------------------------\n date_key           | 2016-09-02                 \n date_id            | 20160902                   \n year               | 2016                       \n quarter            | 3                          \n month              | 9                          \n month_name         | September                  \n day                | 2                          \n day_of_week        | 6                          \n day_name           | Friday                     \n day_of_year        | 246                        \n week_of_year       | 35                         \n is_weekend         | false                      \n quarter_name       | Q3                         \n year_month         | 2016-09                    \n year_quarter       | 2016-Q3                    \n dim_load_timestamp | 2026-02-09 13:23:34.421156 \n-RECORD 2----------------------------------------\n date_key           | 2016-09-03                 \n date_id            | 20160903                   \n year               | 2016                       \n quarter            | 3                          \n month              | 9                          \n month_name         | September                  \n day                | 3                          \n day_of_week        | 7                          \n day_name           | Saturday                   \n day_of_year        | 247                        \n week_of_year       | 35                         \n is_weekend         | true                       \n quarter_name       | Q3                         \n year_month         | 2016-09                    \n year_quarter       | 2016-Q3                    \n dim_load_timestamp | 2026-02-09 13:23:34.421156 \n\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD04 Adding date attributes...\")\n",
    "\n",
    "df_dim_date = df_dates \\\n",
    "    .withColumn(\"date_id\", date_format(col(\"date_key\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "    .withColumn(\"year\", year(col(\"date_key\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"date_key\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"date_key\"))) \\\n",
    "    .withColumn(\"month_name\", date_format(col(\"date_key\"), \"MMMM\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"date_key\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"date_key\"))) \\\n",
    "    .withColumn(\"day_name\", date_format(col(\"date_key\"), \"EEEE\")) \\\n",
    "    .withColumn(\"day_of_year\", dayofyear(col(\"date_key\"))) \\\n",
    "    .withColumn(\"week_of_year\", weekofyear(col(\"date_key\"))) \\\n",
    "    .withColumn(\"is_weekend\", \n",
    "        when(col(\"day_of_week\").isin(1, 7), True).otherwise(False)\n",
    "    ) \\\n",
    "    .withColumn(\"quarter_name\", \n",
    "        expr(\"concat('Q', quarter)\")\n",
    "    ) \\\n",
    "    .withColumn(\"year_month\", date_format(col(\"date_key\"), \"yyyy-MM\")) \\\n",
    "    .withColumn(\"year_quarter\", \n",
    "        expr(\"concat(year, '-Q', quarter)\")\n",
    "    ) \\\n",
    "    .withColumn(\"dim_load_timestamp\", current_timestamp())\n",
    "\n",
    "print(f\"✅ Date dimension created: {df_dim_date.count():,} dates\")\n",
    "\n",
    "# Show sample\n",
    "df_dim_date.limit(3).show(truncate=False, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ed0df5-5154-4965-845e-f368c0d3d940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCBE Writing to: abfss://gold@stgolistmigration.dfs.core.windows.net/dim_date/\n✅ dim_date complete!\n   Rows: 852 dates\n"
     ]
    }
   ],
   "source": [
    "output_path = get_gold_path(\"dim_date\")\n",
    "\n",
    "print(f\"\uD83D\uDCBE Writing to: {output_path}\")\n",
    "\n",
    "df_dim_date.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(\"✅ dim_date complete!\")\n",
    "print(f\"   Rows: {df_dim_date.count():,} dates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209dc349-eb94-4099-9a88-2f0cfe6046fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Verifying...\n✅ Verified: 852 dates\n   Date range: 2016-09-01 to 2018-12-31\n\nSample dates:\n+----------+----+----------+---------+----------+\n|  date_key|year|month_name| day_name|is_weekend|\n+----------+----+----------+---------+----------+\n|2016-09-01|2016| September| Thursday|     false|\n|2016-09-02|2016| September|   Friday|     false|\n|2016-09-03|2016| September| Saturday|      true|\n|2016-09-04|2016| September|   Sunday|      true|\n|2016-09-05|2016| September|   Monday|     false|\n|2016-09-06|2016| September|  Tuesday|     false|\n|2016-09-07|2016| September|Wednesday|     false|\n|2016-09-08|2016| September| Thursday|     false|\n|2016-09-09|2016| September|   Friday|     false|\n|2016-09-10|2016| September| Saturday|      true|\n+----------+----+----------+---------+----------+\n\n\nWeekend vs Weekday:\n+----------+-----+\n|is_weekend|count|\n+----------+-----+\n|      true|  244|\n|     false|  608|\n+----------+-----+\n\n\uD83C\uDF89 dim_date → Gold complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD0D Verifying...\")\n",
    "\n",
    "df_verify = spark.read.format(\"delta\").load(output_path)\n",
    "\n",
    "print(f\"✅ Verified: {df_verify.count():,} dates\")\n",
    "print(f\"   Date range: {df_verify.agg({'date_key': 'min'}).collect()[0][0]} to {df_verify.agg({'date_key': 'max'}).collect()[0][0]}\")\n",
    "\n",
    "print(\"\\nSample dates:\")\n",
    "df_verify.select(\"date_key\", \"year\", \"month_name\", \"day_name\", \"is_weekend\").limit(10).show()\n",
    "\n",
    "print(\"\\nWeekend vs Weekday:\")\n",
    "df_verify.groupBy(\"is_weekend\").count().show()\n",
    "\n",
    "print(\"\uD83C\uDF89 dim_date → Gold complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09_silver_to_gold_dim_date",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}