{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86dc3731-baf8-465d-b45f-db00c9390089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze → Silver: NASA Turbofan Engine Data\n",
    "\n",
    "## Purpose\n",
    "Process NASA C-MAPSS turbofan engine degradation sensor data\n",
    "\n",
    "## Dataset\n",
    "- **Source:** NASA Prognostics Center of Excellence\n",
    "- **Files:** train_FD001-004.txt (training data with run-to-failure cycles)\n",
    "- **Sensors:** 21 sensor readings + operational settings\n",
    "- **Use Case:** Predictive maintenance, RUL prediction\n",
    "\n",
    "## Transformations\n",
    "- Parse space-delimited files\n",
    "- Add proper column names\n",
    "- Filter invalid readings\n",
    "- Normalize sensor IDs\n",
    "- Partition by engine unit\n",
    "\n",
    "**Author:** Kevin  \n",
    "**Date:** Feb 9, 2026\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d418ac1f-9c29-4e7c-af73-8229617fee09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config loaded\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, monotonically_increasing_id, current_timestamp, lit\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "\n",
    "storage_account_name = \"stgolistmigration\"\n",
    "account_key = \"\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "\n",
    "def get_bronze_path(filename):\n",
    "    return f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net/{filename}\"\n",
    "\n",
    "def get_silver_path(table):\n",
    "    return f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/{table}/\"\n",
    "\n",
    "print(\"✅ Config loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db96243-2102-44d3-abbe-2705ad881af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema defined: 26 columns\n"
     ]
    }
   ],
   "source": [
    "# NASA turbofan column schema\n",
    "# Format: unit_id, time_cycle, 3 operational settings, 21 sensor measurements\n",
    "\n",
    "column_names = [\n",
    "    \"unit_id\",           # Engine unit number\n",
    "    \"time_cycle\",        # Operational cycle\n",
    "    \"setting_1\",         # Operational setting 1\n",
    "    \"setting_2\",         # Operational setting 2  \n",
    "    \"setting_3\",         # Operational setting 3\n",
    "    \"sensor_1\",          # Fan inlet temperature\n",
    "    \"sensor_2\",          # LPC outlet temperature\n",
    "    \"sensor_3\",          # HPC outlet temperature\n",
    "    \"sensor_4\",          # LPT outlet temperature\n",
    "    \"sensor_5\",          # Fan inlet pressure\n",
    "    \"sensor_6\",          # Bypass-duct pressure\n",
    "    \"sensor_7\",          # HPC outlet pressure\n",
    "    \"sensor_8\",          # Physical fan speed\n",
    "    \"sensor_9\",          # Physical core speed\n",
    "    \"sensor_10\",         # Engine pressure ratio\n",
    "    \"sensor_11\",         # HPC outlet static pressure\n",
    "    \"sensor_12\",         # Ratio of fuel flow to Ps30\n",
    "    \"sensor_13\",         # Corrected fan speed\n",
    "    \"sensor_14\",         # Corrected core speed\n",
    "    \"sensor_15\",         # Bypass ratio\n",
    "    \"sensor_16\",         # Burner fuel-air ratio\n",
    "    \"sensor_17\",         # Bleed enthalpy\n",
    "    \"sensor_18\",         # Demanded fan speed\n",
    "    \"sensor_19\",         # Demanded corrected fan speed\n",
    "    \"sensor_20\",         # HPT coolant bleed\n",
    "    \"sensor_21\"          # LPT coolant bleed\n",
    "]\n",
    "\n",
    "print(f\"✅ Schema defined: {len(column_names)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799112c7-b3b5-4ea5-b9d8-ee7bcf53fab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCD6 Reading NASA training data...\nMax columns found: 26\n   Reading FD001...\n      Loaded: 20,631 cycles, 27 columns\n   Reading FD002...\n      Loaded: 53,759 cycles, 27 columns\n   Reading FD003...\n      Loaded: 24,720 cycles, 27 columns\n   Reading FD004...\n      Loaded: 61,249 cycles, 27 columns\n\n✅ Total cycles: 160,359\n   Columns: 27\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDCD6 Reading NASA training data...\")\n",
    "\n",
    "# Read all 4 training files\n",
    "datasets = []\n",
    "max_cols = 0\n",
    "\n",
    "# First pass: find max columns\n",
    "for i in range(1, 5):\n",
    "    file_path = get_bronze_path(f\"train_FD00{i}.txt\")\n",
    "    df_temp = spark.read \\\n",
    "        .option(\"delimiter\", \" \") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    # Remove empty columns\n",
    "    actual_cols = [c for c in df_temp.columns if df_temp.select(c).distinct().count() > 1]\n",
    "    max_cols = max(max_cols, len(actual_cols))\n",
    "\n",
    "print(f\"Max columns found: {max_cols}\")\n",
    "\n",
    "# Second pass: read and align all to max columns\n",
    "for i in range(1, 5):\n",
    "    file_path = get_bronze_path(f\"train_FD00{i}.txt\")\n",
    "    \n",
    "    print(f\"   Reading FD00{i}...\")\n",
    "    \n",
    "    df_temp = spark.read \\\n",
    "        .option(\"delimiter\", \" \") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    # Remove empty columns\n",
    "    actual_cols = [c for c in df_temp.columns if df_temp.select(c).distinct().count() > 1]\n",
    "    df_temp = df_temp.select(actual_cols)\n",
    "    \n",
    "    # Rename to standard column names\n",
    "    for idx in range(len(actual_cols)):\n",
    "        if idx < len(column_names):\n",
    "            df_temp = df_temp.withColumnRenamed(df_temp.columns[idx], column_names[idx])\n",
    "    \n",
    "    # Add missing columns with null values\n",
    "    for idx in range(len(actual_cols), len(column_names)):\n",
    "        if column_names[idx] not in df_temp.columns:\n",
    "            df_temp = df_temp.withColumn(column_names[idx], col(\"unit_id\").cast(\"double\") * 0)  # Add as 0.0\n",
    "    \n",
    "    # Add dataset identifier\n",
    "    df_temp = df_temp.withColumn(\"dataset_name\", lit(f\"FD00{i}\"))\n",
    "    \n",
    "    # Ensure same column order\n",
    "    df_temp = df_temp.select(column_names + [\"dataset_name\"])\n",
    "    \n",
    "    datasets.append(df_temp)\n",
    "    print(f\"      Loaded: {df_temp.count():,} cycles, {len(df_temp.columns)} columns\")\n",
    "\n",
    "# Union all datasets\n",
    "df_nasa_bronze = datasets[0]\n",
    "for df in datasets[1:]:\n",
    "    df_nasa_bronze = df_nasa_bronze.union(df)\n",
    "\n",
    "print(f\"\\n✅ Total cycles: {df_nasa_bronze.count():,}\")\n",
    "print(f\"   Columns: {len(df_nasa_bronze.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f5c1f50-9634-4b59-ba2e-254c58087f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Data Quality Check\n================================================================================\n\n1️⃣ RECORD COUNTS:\n+------------+-----+\n|dataset_name|count|\n+------------+-----+\n|FD001       |20631|\n|FD002       |53759|\n|FD003       |24720|\n|FD004       |61249|\n+------------+-----+\n\n\n2️⃣ UNIQUE ENGINES:\nTotal unique engines: 709\n\n3️⃣ CYCLE RANGE:\n+---------------+\n|min(time_cycle)|\n+---------------+\n|              1|\n+---------------+\n\n+---------------+\n|max(time_cycle)|\n+---------------+\n|            543|\n+---------------+\n\n\n4️⃣ SAMPLE DATA:\n-RECORD 0---------------\n unit_id      | 1       \n time_cycle   | 1       \n setting_1    | -7.0E-4 \n setting_2    | -4.0E-4 \n setting_3    | 641.82  \n sensor_1     | 1589.7  \n sensor_2     | 1400.6  \n sensor_3     | 21.61   \n sensor_4     | 554.36  \n sensor_5     | 2388.06 \n sensor_6     | 9046.19 \n sensor_7     | 47.47   \n sensor_8     | 521.66  \n sensor_9     | 2388.02 \n sensor_10    | 8138.62 \n sensor_11    | 8.4195  \n sensor_12    | 392.0   \n sensor_13    | 39.06   \n sensor_14    | 23.419  \n sensor_15    | 0.0     \n sensor_16    | 0.0     \n sensor_17    | 0.0     \n sensor_18    | 0.0     \n sensor_19    | 0.0     \n sensor_20    | 0.0     \n sensor_21    | 0.0     \n dataset_name | FD001   \n-RECORD 1---------------\n unit_id      | 1       \n time_cycle   | 2       \n setting_1    | 0.0019  \n setting_2    | -3.0E-4 \n setting_3    | 642.15  \n sensor_1     | 1591.82 \n sensor_2     | 1403.14 \n sensor_3     | 21.61   \n sensor_4     | 553.75  \n sensor_5     | 2388.04 \n sensor_6     | 9044.07 \n sensor_7     | 47.49   \n sensor_8     | 522.28  \n sensor_9     | 2388.07 \n sensor_10    | 8131.49 \n sensor_11    | 8.4318  \n sensor_12    | 392.0   \n sensor_13    | 39.0    \n sensor_14    | 23.4236 \n sensor_15    | 0.0     \n sensor_16    | 0.0     \n sensor_17    | 0.0     \n sensor_18    | 0.0     \n sensor_19    | 0.0     \n sensor_20    | 0.0     \n sensor_21    | 0.0     \n dataset_name | FD001   \n-RECORD 2---------------\n unit_id      | 1       \n time_cycle   | 3       \n setting_1    | -0.0043 \n setting_2    | 3.0E-4  \n setting_3    | 642.35  \n sensor_1     | 1587.99 \n sensor_2     | 1404.2  \n sensor_3     | 21.61   \n sensor_4     | 554.26  \n sensor_5     | 2388.08 \n sensor_6     | 9052.94 \n sensor_7     | 47.27   \n sensor_8     | 522.42  \n sensor_9     | 2388.03 \n sensor_10    | 8133.23 \n sensor_11    | 8.4178  \n sensor_12    | 390.0   \n sensor_13    | 38.95   \n sensor_14    | 23.3442 \n sensor_15    | 0.0     \n sensor_16    | 0.0     \n sensor_17    | 0.0     \n sensor_18    | 0.0     \n sensor_19    | 0.0     \n sensor_20    | 0.0     \n sensor_21    | 0.0     \n dataset_name | FD001   \n\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD0D Data Quality Check\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1️⃣ RECORD COUNTS:\")\n",
    "df_nasa_bronze.groupBy(\"dataset_name\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"dataset_name\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "print(\"\\n2️⃣ UNIQUE ENGINES:\")\n",
    "total_engines = df_nasa_bronze.select(\"unit_id\", \"dataset_name\").distinct().count()\n",
    "print(f\"Total unique engines: {total_engines}\")\n",
    "\n",
    "print(\"\\n3️⃣ CYCLE RANGE:\")\n",
    "df_nasa_bronze.agg(\n",
    "    {\"time_cycle\": \"min\"}\n",
    ").show()\n",
    "\n",
    "df_nasa_bronze.agg(\n",
    "    {\"time_cycle\": \"max\"}\n",
    ").show()\n",
    "\n",
    "print(\"\\n4️⃣ SAMPLE DATA:\")\n",
    "df_nasa_bronze.limit(3).show(5, truncate=False, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29639298-939c-41c2-9f15-2bb37b590e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Transforming NASA data...\n✅ Transformation complete\n   Silver rows: 160,359\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD04 Transforming NASA data...\")\n",
    "\n",
    "df_nasa_silver = df_nasa_bronze \\\n",
    "    .filter(col(\"unit_id\").isNotNull()) \\\n",
    "    .filter(col(\"time_cycle\").isNotNull()) \\\n",
    "    .withColumn(\"record_id\", monotonically_increasing_id()) \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "silver_count = df_nasa_silver.count()\n",
    "\n",
    "print(f\"✅ Transformation complete\")\n",
    "print(f\"   Silver rows: {silver_count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ef5756-b74e-4206-ab52-26ee749fb7ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCBE Writing to: abfss://silver@stgolistmigration.dfs.core.windows.net/nasa_turbofan_train/\n✅ NASA Silver complete!\n"
     ]
    }
   ],
   "source": [
    "output_path = get_silver_path(\"nasa_turbofan_train\")\n",
    "\n",
    "print(f\"\uD83D\uDCBE Writing to: {output_path}\")\n",
    "\n",
    "df_nasa_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .partitionBy(\"dataset_name\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(\"✅ NASA Silver complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d1dd0f-fecc-4334-98af-805cf2b82504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Verifying...\n✅ Verified: 160,359 sensor readings\n   Unique engines: 4\n\nDataset distribution:\n+------------+-----+\n|dataset_name|count|\n+------------+-----+\n|FD001       |20631|\n|FD002       |53759|\n|FD003       |24720|\n|FD004       |61249|\n+------------+-----+\n\n================================================================================\n\uD83C\uDF89 NASA Bronze → Silver complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD0D Verifying...\")\n",
    "\n",
    "df_verify = spark.read.format(\"delta\").load(output_path)\n",
    "\n",
    "print(f\"✅ Verified: {df_verify.count():,} sensor readings\")\n",
    "print(f\"   Unique engines: {df_verify.select('dataset_name').distinct().count()}\")\n",
    "\n",
    "print(\"\\nDataset distribution:\")\n",
    "df_verify.groupBy(\"dataset_name\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"dataset_name\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\uD83C\uDF89 NASA Bronze → Silver complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_bronze_to_silver_nasa_turbofan",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}